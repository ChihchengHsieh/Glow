{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as Data\n",
    "import torchvision.transforms as T\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser():\n",
    "    #hyperparameters\n",
    "    def __init__(self):\n",
    "        self.n_batch_train = 64 \n",
    "        self.n_batch_test = 64 \n",
    "        self.fmap = 1\n",
    "        self.pmap = 16\n",
    "        self.dal = 1\n",
    "        self.n_batch_init = 256\n",
    "        self.lr = 0.001\n",
    "        self.polyak_epochs = 1\n",
    "        self.epochs_warmup = 10\n",
    "        self.anchor_size = 32\n",
    "        self.n_levels = 3\n",
    "        self.n_sample = 1\n",
    "        self.n_channel = 1\n",
    "        self.n_epoch = 5\n",
    "        self.flow_permutation = 2\n",
    "        self.flow_coupling = 0\n",
    "        self.image_size = 32\n",
    "        self.n_bits_x = 8\n",
    "        self.depth = 32\n",
    "        self.dim = 128\n",
    "        self.y_dim = 10\n",
    "        self.n_y= 10\n",
    "        self.batch_size = 5\n",
    "        self.learntop = True\n",
    "        self.ycond = True\n",
    "        self.weight_y = 0.5\n",
    "hps = Parser()  \n",
    "hps.n_bins = 2. ** hps.n_bits_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "to_img= T.Compose([T.ToPILImage()])\n",
    "to_tensor = T.Compose([T.ToTensor()])\n",
    "load_norm = T.Compose([T.Resize((hps.image_size,hps.image_size)),T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(z, logdet):\n",
    "    zshape = z.size()\n",
    "    z = z.view(-1,zshape[1]*zshape[2]*zshape[3])\n",
    "    logdet = logdet.view(-1,1)\n",
    "    combined = torch.cat([z, logdet], dim = 1)\n",
    "    logdet = combined[:, -1] # Why do we omit the last one?\n",
    "    z = combined[:, :-1].view(-1, zshape[1], zshape[2], zshape[3])\n",
    "    return z, logdet\n",
    "\n",
    "def squeeze2d(x, factor=2):\n",
    "    assert factor >= 1\n",
    "    if factor == 1:\n",
    "        return x\n",
    "    shape = x.size()\n",
    "    height = int(shape[2])\n",
    "    width = int(shape[3])\n",
    "    n_channels = int(shape[1])\n",
    "    assert height % factor == 0 and width % factor == 0\n",
    "    x = x.view( [-1, n_channels, height//factor, factor,\n",
    "                       width//factor, factor ])\n",
    "    x = x.permute([0, 1, 3, 5, 2, 4])\n",
    "    x = x.contiguous().view([-1, n_channels*factor*factor,height//factor, width //factor])\n",
    "    return x\n",
    "\n",
    "def unsqueeze2d(x, factor=2):\n",
    "    assert factor >= 1\n",
    "    if factor == 1:\n",
    "        return x\n",
    "    shape = x.size()\n",
    "    height = int(shape[2])\n",
    "    width = int(shape[3])\n",
    "    n_channels = int(shape[1])\n",
    "    assert n_channels >= 4 and n_channels % 4 == 0\n",
    "    x = x.view(-1, int(n_channels/(factor**2)), height, width, factor, factor)\n",
    "    x = x.permute([0, 1, 4, 2, 5, 3])\n",
    "    x = x.contiguous().view(-1, int(n_channels/(factor**2)),int(height*factor), int(width*factor))\n",
    "    return x\n",
    "\n",
    "def gaussian_diag(mean, logsd):\n",
    "    class o(object):\n",
    "        pass\n",
    "    o.mean = mean\n",
    "    o.logsd = logsd\n",
    "    o.eps = torch.randn(mean.size())\n",
    "    o.sample = mean + torch.exp(logsd) * o.eps\n",
    "    o.sample2 = lambda eps: mean + torch.exp(logsd) * eps\n",
    "    o.logps = lambda x: -0.5 * (torch.log(torch.tensor(2 * np.pi)) + 2. * logsd + (x - mean) ** 2 / torch.exp(2. * logsd))\n",
    "    o.logp = lambda x: flatten_sum(o.logps(x))\n",
    "    o.get_eps = lambda x: (x - mean) / torch.exp(logsd)\n",
    "    return o\n",
    "\n",
    "def flatten_sum(logps):\n",
    "    if len(logps.size()) == 2:\n",
    "        return logps.sum(1)\n",
    "    elif len(logps.size()) == 4:\n",
    "        return logps.sum(-1).sum(-1).sum(-1)\n",
    "    else:\n",
    "        raise Exception()\n",
    "        \n",
    "def preprocess(x):\n",
    "    x = x.float()\n",
    "    if hps.n_bits_x < 8:\n",
    "        x = (x / 2 ** (8 - hps.n_bits_x)).int()\n",
    "    x = x / hps.n_bins - .5\n",
    "    return x\n",
    "\n",
    "def same_pad(kernel_size):\n",
    "    return math.ceil((kernel_size-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actnorm(nn.Module):\n",
    "    def __init__(self, scale = 1, logscale_factor = 3):\n",
    "        super(Actnorm, self).__init__()\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.logscale_factor = logscale_factor\n",
    "        \n",
    "        \n",
    "    def forward(self, x, logdet = None, reverse = False):\n",
    "        self.reverse = reverse\n",
    "        if not self.reverse:\n",
    "            x = self.actnorm_center(x)\n",
    "            x = self.actnorm_scale(x,logdet)\n",
    "            if type(logdet) != type(None):\n",
    "                x, logdet = x \n",
    "        else: \n",
    "            x = self.actnorm_scale(x, logdet)\n",
    "            if type(logdet) != type(None):\n",
    "                x, logdet = x\n",
    "            x = self.actnorm_center(x)\n",
    "        \n",
    "        if type(logdet) != type(None):\n",
    "            return x, logdet\n",
    "        return x\n",
    "    \n",
    "    def actnorm_center(self,x):\n",
    "        \n",
    "        if len(x.size()) == 2:\n",
    "            x_mean = x.mean(1, True)\n",
    "            b = nn.Parameter(x_mean)\n",
    "        if len(x.size()) == 4:\n",
    "            x_mean = x.mean(1, True)\n",
    "            b = nn.Parameter(x_mean)\n",
    "            \n",
    "        if not self.reverse:\n",
    "            x += b\n",
    "        else:\n",
    "            x -= b \n",
    "                \n",
    "        return x \n",
    "    \n",
    "    def actnorm_scale(self,x, logdet= None, logscale_factor = 3):\n",
    "        \n",
    "        if len(x.size()) == 2:\n",
    "            x_var = torch.mean(x**2,1, keepdim=True)\n",
    "            logdet_factor = 1\n",
    "            _shape = (1, x.size(1))\n",
    "\n",
    "        elif len(x.size()) == 4:\n",
    "            x_var = torch.mean(x**2,1, keepdim=True)\n",
    "            logdet_factor = x.size(2)*x.size(3)\n",
    "            _shape = (1, x.size(1), 1, 1)\n",
    "            \n",
    "        logs = nn.Parameter(torch.log(self.scale/(torch.sqrt(x_var)+1e-6))/logscale_factor)*logscale_factor\n",
    "        \n",
    "        if not self.reverse:\n",
    "            x = x * logs.exp()\n",
    "        else:\n",
    "            x = x * (-logs).exp()\n",
    "            \n",
    "        if type(logdet) != type(None):\n",
    "            dlogdet = torch.mean(logs) * logdet_factor\n",
    "            if self.reverse:\n",
    "                dlogdet *= -1\n",
    "            return x, logdet + dlogdet\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible_1x1_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Invertible_1x1_conv, self).__init__()\n",
    "        self.init = True\n",
    "        \n",
    "        \n",
    "    def forward(self, z , logdet, reverse=False):\n",
    "        \n",
    "        if self.init:\n",
    "            self._def_model(z)\n",
    "        \n",
    "        dlogdet = torch.log(abs(torch.det(\n",
    "                self.w))) * z.size(2)* z.size(3)\n",
    "        \n",
    "        if not reverse:\n",
    "            z = F.conv2d(z, self.w.view(self.dim, self.dim, 1, 1), self.b)\n",
    "            logdet += dlogdet\n",
    "            \n",
    "            return z, logdet\n",
    "        \n",
    "        else:\n",
    "            z = z = F.conv2d(z, self.w.inverse().view(self.dim, self.dim, 1, 1), self.b)\n",
    "            logdet += dlogdet\n",
    "            \n",
    "            return z, logdet\n",
    "        \n",
    "    def _def_model(self, z):\n",
    "        self.dim = z.size(1)\n",
    "        self.w = nn.Parameter(torch.tensor(np.linalg.qr(np.random.randn(self.dim, self.dim))[0].astype('float32')))\n",
    "        self.b = nn.Parameter(torch.zeros(self.dim))\n",
    "        self.init = False\n",
    "        #print(self.__class__.__name__,' initialised')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, weightnorm =True, actnorm=True, scale=1.):\n",
    "        super(linear, self).__init__()\n",
    "        self.weightnorm = weightnorm\n",
    "        self.actnorm = actnorm\n",
    "        self.scale = scale\n",
    "        self.l = nn.Linear(in_dim, out_dim)\n",
    "        self.an = Actnorm(scale)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.weightnorm:\n",
    "            torch.nn.utils.clip_grad_norm_(self.l.parameters(),1,2)\n",
    "        x = self.l(x)\n",
    "        \n",
    "        if self.actnorm:\n",
    "            x = self.an(x)\n",
    "        return x\n",
    "    \n",
    "class linear_zeros(nn.Module):\n",
    "    def __init__(self, out_dim, logscale_factor = 3. ):\n",
    "        super(linear_zeros, self).__init__()\n",
    "        \n",
    "        self.init = True\n",
    "        self.out_dim = out_dim\n",
    "        self.logscale_factor = logscale_factor\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.init:\n",
    "            self._init_model(x)\n",
    "        \n",
    "        x = self.l(x)\n",
    "        x *= torch.exp(self.logs)\n",
    "        return x\n",
    "    \n",
    "    def _init_model(self,x):\n",
    "        self.l = nn.Linear(x.size(1), self.out_dim)\n",
    "        self.l.weight.data.fill_(0)\n",
    "        self.l.bias.data.fill_(0)\n",
    "        self.logs = nn.Parameter(torch.zeros(1,self.out_dim))* self.logscale_factor\n",
    "        self.init = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class f(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(f, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Conv2dBlock(out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2dBlock(out_dim, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d_zerosBlock(out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, h):\n",
    "        \n",
    "        return self.model(h)\n",
    "            \n",
    "\n",
    "        \n",
    "class Conv2d_zerosBlock(nn.Module):\n",
    "    def __init__(self, out_dim, kernel_size = 3, logscale_factor = 3 ):\n",
    "        super(Conv2d_zerosBlock, self).__init__()\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.logscale_factor = logscale_factor\n",
    "        self.init = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.init:\n",
    "            self._init_model(x)\n",
    "        \n",
    "        x = self.conv2d(x)\n",
    "        x *= torch.exp(self.logs.expand_as(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _init_model(self, x):\n",
    "        in_dim = x.size(1)\n",
    "        p = same_pad(self.kernel_size)\n",
    "        self.conv2d = nn.Conv2d(in_dim, self.out_dim , self.kernel_size ,1, p)\n",
    "        self.logs = nn.Parameter(torch.zeros(1,self.out_dim, 1, 1)) * self.logscale_factor \n",
    "        nn.init.constant_(self.conv2d.weight,0)\n",
    "        nn.init.constant_(self.conv2d.bias,0)\n",
    "        self.init = False\n",
    "        #print(self.__class__.__name__,' initialised')\n",
    "        \n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, out_dim, kernel_size=3, weightnorm = False, actnorm = True):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.weightnorm = weightnorm\n",
    "        self.actnorm = actnorm\n",
    "        self.k = kernel_size\n",
    "        self.out= out_dim\n",
    "        self.actnorm = Actnorm()\n",
    "        \n",
    "        self.init = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.init:\n",
    "            self._init_model(x)\n",
    "        \n",
    "        if self.weightnorm:\n",
    "            torch.nn.utils.clip_grad_norm_(self.conv2d.parameters(),1,2)\n",
    "            \n",
    "        x = self.conv2d(x)\n",
    "        \n",
    "        if self.actnorm:\n",
    "            x = self.actnorm(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def _init_model(self,x):\n",
    "        self.in_dim = x.size(1)\n",
    "        p = same_pad(self.k)\n",
    "        self.conv2d = nn.Conv2d(self.in_dim, self.out, self.k, 1,p)\n",
    "        self.init = False\n",
    "        #print(self.__class__.__name__,' initialised')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prior, self).__init__()\n",
    "        self.n_z = hps.top_shape[0]\n",
    "        \n",
    "        self.conv2d = Conv2d_zerosBlock(2*self.n_z)\n",
    "        self.linear = linear_zeros(2*self.n_z)\n",
    "        \n",
    "        \n",
    "            \n",
    "    def forward(self, y_onehot):\n",
    "        \n",
    "        h = torch.zeros([y_onehot.size(0)]+[2*self.n_z]+hps.top_shape[1:])\n",
    "        if hps.learntop:\n",
    "            h = self.conv2d(h)\n",
    "        if hps.ycond:\n",
    "            h += self.linear(y_onehot).view(-1,self.n_z*2, 1,1)\n",
    "            \n",
    "        pz = gaussian_diag(h[:,:self.n_z,:,:],h[:,self.n_z:,:,:])\n",
    "        \n",
    "        def logp(z1):\n",
    "            objective = pz.logp(z1)\n",
    "            return objective\n",
    "\n",
    "        def sample(eps_std=None):\n",
    "            if eps_std is not None:\n",
    "                z = pz.sample2(pz.eps * eps_std.view([-1, 1, 1, 1]))\n",
    "            else:\n",
    "                z = pz.sample\n",
    "\n",
    "            return z\n",
    "\n",
    "        return logp, sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Split2d, self).__init__()\n",
    "        '''\n",
    "        dim -> 2 x dim\n",
    "        '''\n",
    "        \n",
    "        self.init = True\n",
    "        \n",
    "    def forward(self, z, objective = 0., eps_std=None, reverse = False):\n",
    "        \n",
    "        if self.init:\n",
    "            self._init_model(z)\n",
    "\n",
    "        if not reverse:\n",
    "            n_z = z.size(1)\n",
    "\n",
    "            z1 = z[:, :n_z // 2, :, :]\n",
    "            z2 = z[:, n_z // 2:, :, : ]\n",
    "            pz = self.split2d_prior(z1)\n",
    "            objective += pz.logp(z2)\n",
    "            z1 = squeeze2d(z1)\n",
    "            return z1, objective\n",
    "        else:\n",
    "            z1 = unsqueeze2d(z)\n",
    "            #print(z1.shape)\n",
    "            pz = self.split2d_prior(z1)\n",
    "            z2 = pz.sample\n",
    "            if type(eps_std) is not type(None):\n",
    "                z2 = pz.sample2(pz.eps * eps_std.view([-1, 1, 1, 1]))\n",
    "            z = torch.cat([z1,z2],1)\n",
    "            return z\n",
    "                \n",
    "    def split2d_prior(self, z):\n",
    "        h = self.conv_zero(z)\n",
    "        mean = h[:, 0::2, :, :]\n",
    "        logs = h[:, 1::2, :, :]\n",
    "        return gaussian_diag(mean, logs)\n",
    "\n",
    "    def _init_model(self,z):\n",
    "        self.conv_zero = Conv2d_zerosBlock(z.size(1))\n",
    "        self.init = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevNet_step(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevNet_step, self).__init__()\n",
    "        self.invertible1x1 = Invertible_1x1_conv()\n",
    "        self.actnorm = Actnorm()\n",
    "        self.init = True\n",
    "        \n",
    "        \n",
    "    def forward(self, z, logdet, reverse=False):\n",
    "        \n",
    "        if self.init:\n",
    "            self._init_model(z)\n",
    "        \n",
    "        if not reverse:\n",
    "            \n",
    "            z, logdet = self.actnorm(z, logdet)\n",
    "            \n",
    "            z, logdet = self.invertible1x1(z, logdet)\n",
    "            z1 = z[:,:z.size(1)//2,:,:]\n",
    "            z2 = z[:,z.size(1)//2:,:,:]\n",
    "            \n",
    "            z2 += self.ff(z1)\n",
    "            \n",
    "            \n",
    "            z = torch.cat([z1,z2],1)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            z1 = z[:,:z.size(1)//2,:,:]\n",
    "            z2 = z[:,z.size(1)//2:,:,:]\n",
    "            \n",
    "            z2 -= self.ff(z1)\n",
    "            \n",
    "            \n",
    "            z = torch.cat([z1,z2],1)\n",
    "            \n",
    "            z, logdet = self.invertible1x1(z, logdet, True)\n",
    "            \n",
    "            z, logdet = self.actnorm(z, logdet, True)\n",
    "            \n",
    "        return z, logdet\n",
    "    def _init_model(self,z):\n",
    "        self.ff = f(z.size(1)//2)\n",
    "        self.init = False\n",
    "        #print(self.__class__.__name__,' initialised')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevNet2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevNet2d, self).__init__()\n",
    "        self.revnet2d_seq = nn.ModuleList()\n",
    "        for i in range(hps.depth):\n",
    "            self.revnet2d_seq.extend([RevNet_step()])\n",
    "            \n",
    "    def forward(self,z, logdet, reverse=False):\n",
    "        \n",
    "        if not reverse:\n",
    "            for i,m in enumerate(self.revnet2d_seq):\n",
    "                z, logdet = checkpoint(z , logdet)\n",
    "                z, logdet = m(z, logdet, False)\n",
    "            z, logdet = checkpoint(z, logdet)\n",
    "            \n",
    "        else: \n",
    "            for i,m in enumerate(reversed(self.revnet2d_seq)):\n",
    "                z, logdet = m(z, logdet, True)\n",
    "        \n",
    "        return z, logdet\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Codec, self).__init__()\n",
    "        \n",
    "        self.revnet_seq = nn.ModuleList()\n",
    "        self.split2d_seq = nn.ModuleList()\n",
    "        for i in range(hps.n_levels):\n",
    "            self.revnet_seq.extend([RevNet2d()])\n",
    "\n",
    "\n",
    "        for i in range(hps.n_levels-1):\n",
    "            self.split2d_seq.extend([Split2d()])\n",
    "\n",
    "    def forward(self,):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def encode(self, z, objective):\n",
    "        \n",
    "        for i in range(hps.n_levels):\n",
    "            z, objective = self.revnet_seq[i](z, objective)\n",
    "            if i < hps.n_levels - 1:\n",
    "                z, objective = self.split2d_seq[i](z, objective)\n",
    "        \n",
    "        return z, objective\n",
    "    \n",
    "    def decode(self, z, eps_std):\n",
    "        \n",
    "        for i in reversed(range(hps.n_levels)):\n",
    "            if i< hps.n_levels -1 :\n",
    "                z = self.split2d_seq[i](z,eps_std=eps_std, reverse= True)\n",
    "                #print(z.shape)\n",
    "            z,_ = self.revnet_seq[i](z,0, reverse=True)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _f_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_f_loss, self).__init__()\n",
    "        \n",
    "        self.codec = Codec()\n",
    "#         self.pr = prior()\n",
    "        self.l_zero = linear_zeros(hps.n_y)\n",
    "        self.sm = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, x,y, is_training=True):\n",
    "    \n",
    "        y_onehot = torch.zeros(y.size(0),hps.y_dim)\n",
    "        #print(y.long().view(-1,1).shape, y_onehot.shape)\n",
    "        y_onehot.scatter_(1, y.long().view(-1,1),1)\n",
    "\n",
    "        objective = torch.zeros_like(x)[:,0,0,0]\n",
    "        #z = preprocess(x)\n",
    "        z = x\n",
    "        z += torch.rand_like(z)\n",
    "\n",
    "        objective += - torch.log(torch.tensor(hps.n_bins)) * (z[0].nelement())\n",
    "\n",
    "        # Encode \n",
    "\n",
    "        z = squeeze2d(z, 2)\n",
    "        z, objective = self.codec.encode(z, objective)\n",
    "        hps.top_shape = list(z.size()[1:])\n",
    "\n",
    "\n",
    "        # Prior\n",
    "        try:\n",
    "            logp, _ = self.pr(y_onehot)\n",
    "        except:\n",
    "            self.pr = prior()\n",
    "            logp, _ = self.pr(y_onehot)\n",
    "            \n",
    "            \n",
    "        objective += logp(z)\n",
    "        \n",
    "        # Generative loss\n",
    "        \n",
    "        nobj = - objective\n",
    "        bits_x = nobj / torch.tensor((np.log(2.) * x[0].nelement()))#  bits per subpixel\n",
    "                                     \n",
    "                                     \n",
    "        # Predictive loss\n",
    "        if hps.weight_y > 0 and hps.ycond:\n",
    "\n",
    "            # Classification loss\n",
    "            h_y = z.sum(-1).sum(-1)\n",
    "            y_logits = self.l_zero(h_y)\n",
    "\n",
    "            bits_y = self.sm(y_logits, y.long())/torch.tensor(2.).log()\n",
    "            \n",
    "\n",
    "            # Classification accuracy\n",
    "                                     \n",
    "            y_predicted = torch.max(y_logits,dim = 1)[1].int()\n",
    "            classification_error = 1 - (torch.eq(y_predicted, y.int())).float()\n",
    "                                     \n",
    "        else:\n",
    "            bits_y = torch.zeros_like(bits_x)\n",
    "            classification_error = torch.ones_like(bits_x)\n",
    "        return bits_x, bits_y, classification_error\n",
    "    \n",
    "    def f_decode(self,y,eps_std):\n",
    "        \n",
    "        y_onehot = torch.zeros(y.size(0),hps.y_dim)\n",
    "        y_onehot.scatter_(1, y.long().view(-1,1),1)\n",
    "        \n",
    "        _,sample = self.pr(y_onehot)\n",
    "        z = sample(eps_std)\n",
    "        \n",
    "        z = self.codec.decode(z, eps_std)\n",
    "        \n",
    "        z = unsqueeze2d(z,2)\n",
    "        \n",
    "        #x = postprocess(z)\n",
    "        \n",
    "        return z\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Glow, self).__init__()\n",
    "        self.f = _f_loss() # hps.batch_size\n",
    "        self.f(torch.ones(hps.batch_size,hps.n_channel,hps.image_size,hps.image_size),torch.ones(hps.batch_size),True) # pre_warm, the model \n",
    "        self.optim = optim.Adam(self.parameters(),lr = hps.lr)\n",
    "        \n",
    "    def forward(self,x,y,is_training=True):\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        bits_x, bits_y, pred_loss = self.f(x,y, is_training)\n",
    "        local_loss = torch.mean(bits_x + hps.weight_y * bits_y)\n",
    "        \n",
    "        stats = [local_loss, bits_x, bits_y, pred_loss]\n",
    "        self.global_stats = [torch.mean(i) for i in stats]\n",
    "        \n",
    "        local_loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def sample(self, temperatures = [0.25]):\n",
    "        #[0., .25, .5, .6, .7, .8, .9, 1.]\n",
    "        x_sample = []\n",
    "        rows = 10\n",
    "        cols = rows\n",
    "        n_batch = rows*cols\n",
    "        y = np.asarray([_y % hps.n_y for _y in (\n",
    "            list(range(cols)) * rows)], dtype='int32')\n",
    "        for i in temperatures:\n",
    "            x_sample.append(self.f.f_decode(torch.tensor(y),torch.tensor([i]*n_batch).float().detach()))\n",
    "        return torch.stack(x_sample)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    " \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(datasets.MNIST('./data/mnist',train=True,\n",
    "                    download=True,transform=load_norm),\n",
    "                    batch_size=hps.batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Glow()\n",
    "epoch = 0\n",
    "all_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.StepLR(g.optim,10000,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step [1] | lr [0.0010] | L_Loss: [14.742] | X_Loss: [13.081] | Y_Loss: [3.322] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [2] | lr [0.0010] | L_Loss: [14.391] | X_Loss: [12.741] | Y_Loss: [3.300] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [3] | lr [0.0010] | L_Loss: [14.331] | X_Loss: [12.540] | Y_Loss: [3.582] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [4] | lr [0.0010] | L_Loss: [14.216] | X_Loss: [12.441] | Y_Loss: [3.551] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [5] | lr [0.0010] | L_Loss: [14.108] | X_Loss: [12.269] | Y_Loss: [3.677] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [6] | lr [0.0010] | L_Loss: [13.773] | X_Loss: [12.083] | Y_Loss: [3.380] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [7] | lr [0.0010] | L_Loss: [13.487] | X_Loss: [11.873] | Y_Loss: [3.228] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [8] | lr [0.0010] | L_Loss: [13.334] | X_Loss: [11.658] | Y_Loss: [3.352] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [9] | lr [0.0010] | L_Loss: [13.162] | X_Loss: [11.524] | Y_Loss: [3.275] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [10] | lr [0.0010] | L_Loss: [13.086] | X_Loss: [11.410] | Y_Loss: [3.352] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [11] | lr [0.0010] | L_Loss: [13.008] | X_Loss: [11.270] | Y_Loss: [3.475] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [12] | lr [0.0010] | L_Loss: [12.755] | X_Loss: [11.173] | Y_Loss: [3.163] | P_Loss: [0.600] | Time: 0.2s\n",
      "| Step [13] | lr [0.0010] | L_Loss: [12.720] | X_Loss: [11.032] | Y_Loss: [3.376] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [14] | lr [0.0010] | L_Loss: [12.470] | X_Loss: [10.831] | Y_Loss: [3.278] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [15] | lr [0.0010] | L_Loss: [12.388] | X_Loss: [10.715] | Y_Loss: [3.346] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [16] | lr [0.0010] | L_Loss: [12.223] | X_Loss: [10.581] | Y_Loss: [3.283] | P_Loss: [0.600] | Time: 0.2s\n",
      "| Step [17] | lr [0.0010] | L_Loss: [11.991] | X_Loss: [10.374] | Y_Loss: [3.234] | P_Loss: [0.600] | Time: 0.2s\n",
      "| Step [18] | lr [0.0010] | L_Loss: [12.122] | X_Loss: [10.286] | Y_Loss: [3.672] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [19] | lr [0.0010] | L_Loss: [11.755] | X_Loss: [10.120] | Y_Loss: [3.271] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [20] | lr [0.0010] | L_Loss: [11.619] | X_Loss: [9.972] | Y_Loss: [3.293] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [21] | lr [0.0010] | L_Loss: [11.518] | X_Loss: [9.837] | Y_Loss: [3.362] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [22] | lr [0.0010] | L_Loss: [11.350] | X_Loss: [9.694] | Y_Loss: [3.311] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [23] | lr [0.0010] | L_Loss: [11.306] | X_Loss: [9.607] | Y_Loss: [3.399] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [24] | lr [0.0010] | L_Loss: [11.172] | X_Loss: [9.446] | Y_Loss: [3.451] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [25] | lr [0.0010] | L_Loss: [11.013] | X_Loss: [9.326] | Y_Loss: [3.374] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [26] | lr [0.0010] | L_Loss: [10.851] | X_Loss: [9.216] | Y_Loss: [3.270] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [27] | lr [0.0010] | L_Loss: [10.972] | X_Loss: [9.119] | Y_Loss: [3.707] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [28] | lr [0.0010] | L_Loss: [10.751] | X_Loss: [9.052] | Y_Loss: [3.398] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [29] | lr [0.0010] | L_Loss: [10.721] | X_Loss: [8.964] | Y_Loss: [3.515] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [30] | lr [0.0010] | L_Loss: [10.198] | X_Loss: [8.857] | Y_Loss: [2.682] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [31] | lr [0.0010] | L_Loss: [10.238] | X_Loss: [8.752] | Y_Loss: [2.971] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [32] | lr [0.0010] | L_Loss: [10.328] | X_Loss: [8.659] | Y_Loss: [3.338] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [33] | lr [0.0010] | L_Loss: [10.143] | X_Loss: [8.547] | Y_Loss: [3.193] | P_Loss: [0.800] | Time: 0.3s\n",
      "| Step [34] | lr [0.0010] | L_Loss: [10.458] | X_Loss: [8.454] | Y_Loss: [4.008] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [35] | lr [0.0010] | L_Loss: [10.001] | X_Loss: [8.372] | Y_Loss: [3.258] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [36] | lr [0.0010] | L_Loss: [10.003] | X_Loss: [8.272] | Y_Loss: [3.462] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [37] | lr [0.0010] | L_Loss: [9.935] | X_Loss: [8.201] | Y_Loss: [3.468] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [38] | lr [0.0010] | L_Loss: [9.782] | X_Loss: [8.104] | Y_Loss: [3.355] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [39] | lr [0.0010] | L_Loss: [9.643] | X_Loss: [8.009] | Y_Loss: [3.268] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [40] | lr [0.0010] | L_Loss: [9.727] | X_Loss: [7.876] | Y_Loss: [3.702] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [41] | lr [0.0010] | L_Loss: [9.363] | X_Loss: [7.761] | Y_Loss: [3.204] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [42] | lr [0.0010] | L_Loss: [9.509] | X_Loss: [7.696] | Y_Loss: [3.625] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [43] | lr [0.0010] | L_Loss: [9.353] | X_Loss: [7.602] | Y_Loss: [3.502] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [44] | lr [0.0010] | L_Loss: [9.247] | X_Loss: [7.511] | Y_Loss: [3.472] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [45] | lr [0.0010] | L_Loss: [9.136] | X_Loss: [7.400] | Y_Loss: [3.470] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [46] | lr [0.0010] | L_Loss: [8.925] | X_Loss: [7.301] | Y_Loss: [3.247] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [47] | lr [0.0010] | L_Loss: [8.930] | X_Loss: [7.209] | Y_Loss: [3.441] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [48] | lr [0.0010] | L_Loss: [8.904] | X_Loss: [7.114] | Y_Loss: [3.580] | P_Loss: [1.000] | Time: 0.3s\n",
      "| Step [49] | lr [0.0010] | L_Loss: [8.769] | X_Loss: [7.007] | Y_Loss: [3.525] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [50] | lr [0.0010] | L_Loss: [8.611] | X_Loss: [6.913] | Y_Loss: [3.396] | P_Loss: [0.800] | Time: 0.2s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step [51] | lr [0.0010] | L_Loss: [8.489] | X_Loss: [6.818] | Y_Loss: [3.342] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [52] | lr [0.0010] | L_Loss: [8.388] | X_Loss: [6.739] | Y_Loss: [3.298] | P_Loss: [1.000] | Time: 0.2s\n",
      "| Step [53] | lr [0.0010] | L_Loss: [8.359] | X_Loss: [6.652] | Y_Loss: [3.415] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [54] | lr [0.0010] | L_Loss: [8.270] | X_Loss: [6.590] | Y_Loss: [3.360] | P_Loss: [0.800] | Time: 0.2s\n",
      "| Step [55] | lr [0.0010] | L_Loss: [8.091] | X_Loss: [6.472] | Y_Loss: [3.238] | P_Loss: [0.800] | Time: 0.2s\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9b9472c4f564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mall_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m55\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < hps.n_epoch:\n",
    "    for i, (img, y) in enumerate(training_loader):    \n",
    "        \n",
    "        start_t = time.time()\n",
    "        g(img,y,True)\n",
    "        end_t = time.time()\n",
    "        \n",
    "#         scheduler.step()\n",
    "        \n",
    "        print('| Step [%d] | lr [%.4f] | L_Loss: [%.3f] | X_Loss: [%.3f] | Y_Loss: [%.3f] | P_Loss: [%.3f] | Time: %.1fs' %\\\n",
    "              ( all_steps, g.optim.param_groups[0]['lr'], g.global_stats[0].item(), g.global_stats[1].item(),\n",
    "               g.global_stats[2].item(),g.global_stats[3].item(),\n",
    "               end_t - start_t))\n",
    "\n",
    "\n",
    "        if all_steps % 10 == 0: #args.show_freq\n",
    "            fig=plt.figure(figsize=(8, 8))\n",
    "            fig.add_subplot(1,3,1)\n",
    "            plt.imshow(to_img(g.sample()[0][0].cpu()*0.5+0.5))\n",
    "            plt.show()\n",
    "#             if all_steps % args.img_save_freq ==0: # args.img_save_freq\n",
    "#                 gan.image_save(all_steps)\n",
    "#                 gan.plot_all_loss('Training')\n",
    "#                 if all_steps % args.model_save_freq == 0: #args.model_save_freq\n",
    "#                     gan.model_save(all_steps)\n",
    "        all_steps += 1\n",
    "        if all_steps > 55:\n",
    "            raise StopIteration\n",
    "    epoch +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
