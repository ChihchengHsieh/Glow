{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Remove the normalization \n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Parser(object):\n",
    "    \n",
    "    ## HyperParams ##\n",
    "    \n",
    "    def __init__(self, name ):\n",
    "        class Type(object):\n",
    "            pass\n",
    "        \n",
    "        self.Glow = Type()\n",
    "        self.Glow.hidden_channels = 96\n",
    "        self.Glow.flow_depth = 2\n",
    "        self.Glow.flow_levels = 3\n",
    "        self.Glow.actnorm_scale = 1.\n",
    "        self.Glow.logscale_factor = 3.\n",
    "        self.Glow.flow_permutation = \"invconv\"\n",
    "        self.Glow.flow_coupling = \"affine\"\n",
    "        self.Glow.LU_decomposed = False\n",
    "        self.Glow.learn_top = False\n",
    "        self.Glow.y_condition = True\n",
    "        self.Glow.K = 32\n",
    "        self.Glow.L = 3\n",
    "        self.Glow.model_name = 'Glow'\n",
    "                \n",
    "        self.Data = Type()\n",
    "        self.Data.name = name\n",
    "        \n",
    "        \n",
    "        self.Optim = Type()\n",
    "        self.Optim.lr = 1e-3\n",
    "        self.Optim.beta1 = 0.9\n",
    "        self.Optim.beta2 = 0.999\n",
    "        \n",
    "        self.Train = Type()  \n",
    "        self.Train.batch_size = 64\n",
    "        self.Train.n_epoch = 30\n",
    "        self.Train.with_attention_guide = True\n",
    "        self.Train.max_grad_clip = 5\n",
    "        self.Train.max_grad_norm = 100\n",
    "        self.Train.time_steps_mask = True\n",
    "        self.Train.scalar_log_gap = 100\n",
    "        self.Train.weight_y = 0.5\n",
    "        self.Train.img_show_freq = 50\n",
    "        self.Train.img_save_freq = 100\n",
    "        self.Train.model_save_freq = 1000\n",
    "        \n",
    "        if self.Data.name == 'mnist':\n",
    "            self.Glow.in_channels = 1 \n",
    "            self.Glow.image_shape = (1, 32, 32) \n",
    "            self.Glow.y_classes = 10 \n",
    "            self.Data.img_size = 32 \n",
    "            \n",
    "        elif self.Data.name == 'celeba':\n",
    "            self.Glow.in_channels = 3 \n",
    "            self.Glow.image_shape = (3, 64, 64) \n",
    "            self.Glow.y_classes = 2 \n",
    "            self.Data.img_size = 64 \n",
    "            self.Data.train_img_path = \"./data/celeba/\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "hparams = Parser('celeba')\n",
    "\n",
    "img_path = './'+ hparams.Glow.model_name +'/Img/'\n",
    "model_path = './'+ hparams.Glow.model_name +'/Model/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if not os.path.exists(img_path):\n",
    "    os.makedirs(img_path)\n",
    "\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        \n",
    "        self.image_transform = T.Compose([\n",
    "            T.Resize((hparams.Data.img_size,hparams.Data.img_size)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        \n",
    "        with open(\"Path_Male.pickle\", 'rb') as f:\n",
    "            self.path ,self.male= pickle.load(f)\n",
    "            print('Loaded')\n",
    "        self.path = self.path[:-2000] if mode == 'train' else self.path[-2000:]\n",
    "        self.male = self.male[:-2000] if mode == 'train' else self.male[-2000:]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        idx = index % len(self.path)\n",
    "        img = self.image_transform(Image.open(os.path.join(hparams.Data.train_img_path,\n",
    "                                                             self.path[idx])))\n",
    "        \n",
    "        is_male = self.male[idx]\n",
    "        return img, is_male\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "\n",
    "def thops_onehot(y, num_classes):\n",
    "    \n",
    "    y_onehot = torch.zeros(y.size(0), num_classes).to(y.device)\n",
    "    if len(y.size()) == 1:\n",
    "        y_onehot = y_onehot.scatter_(1, y.unsqueeze(-1), 1)\n",
    "    elif len(y.size()) == 2:\n",
    "        y_onehot = y_onehot.scatter_(1, y, 1)\n",
    "    else:\n",
    "        raise ValueError(\"[onehot]: y should be in shape [B], or [B, C]\")\n",
    "    return y_onehot\n",
    "\n",
    "def thops_sum(tensor, dim=None, keepdim=False):\n",
    "    if dim is None:\n",
    "        # sum up all dim\n",
    "        return torch.sum(tensor)\n",
    "    else:\n",
    "        if isinstance(dim, int):\n",
    "            dim = [dim]\n",
    "        dim = sorted(dim)\n",
    "        for d in dim:\n",
    "            tensor = tensor.sum(dim=d, keepdim=True)\n",
    "        if not keepdim:\n",
    "            for i, d in enumerate(dim):\n",
    "                tensor.squeeze_(d-i)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def thops_mean(tensor, dim=None, keepdim=False):\n",
    "    if dim is None:\n",
    "        # mean all dim\n",
    "        return torch.mean(tensor)\n",
    "    else:\n",
    "        if isinstance(dim, int):\n",
    "            dim = [dim]\n",
    "        dim = sorted(dim)\n",
    "        for d in dim:\n",
    "            tensor = tensor.mean(dim=d, keepdim=True)\n",
    "        if not keepdim:\n",
    "            for i, d in enumerate(dim):\n",
    "                tensor.squeeze_(d-i)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def thops_split_feature(tensor, type=\"split\"):\n",
    "    \"\"\"\n",
    "    type = [\"split\", \"cross\"]\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    if type == \"split\":\n",
    "        return tensor[:, :C // 2, ...], tensor[:, C // 2:, ...]\n",
    "    elif type == \"cross\":\n",
    "        return tensor[:, 0::2, ...], tensor[:, 1::2, ...]\n",
    "\n",
    "\n",
    "def thops_cat_feature(tensor_a, tensor_b):\n",
    "    return torch.cat((tensor_a, tensor_b), dim=1)\n",
    "\n",
    "\n",
    "def thops_pixels(tensor):\n",
    "    return int(tensor.size(2) * tensor.size(3))\n",
    "\n",
    "\n",
    "class _ActNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Activation Normalization\n",
    "    Initialize the bias and scale with a given minibatch,\n",
    "    so that the output per-channel have zero mean and unit variance for that.\n",
    "    After initialization, `bias` and `logs` will be trained as parameters.\n",
    "    \"A trainable normalisation, similar to BN but use less params\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, scale=1.):\n",
    "        super().__init__()\n",
    "        # register mean and scale\n",
    "        size = [1, num_features, 1, 1]\n",
    "        self.register_parameter(\"bias\", nn.Parameter(torch.zeros(*size))) # turn it to self.bias = nn.Parameter\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(*size)))\n",
    "        self.num_features = num_features\n",
    "        self.scale = float(scale)\n",
    "        self.inited = False\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        return NotImplemented\n",
    "\n",
    "    def initialize_parameters(self, input):\n",
    "        self._check_input_dim(input)\n",
    "        if not self.training:\n",
    "            return\n",
    "        assert input.device == self.bias.device\n",
    "        with torch.no_grad():\n",
    "            # Similar to BN\n",
    "            bias = thops_mean(input, dim=[0, 2, 3], keepdim=True) * -1.0\n",
    "            vars = thops_mean((input + bias) ** 2, dim=[0, 2, 3], keepdim=True)\n",
    "            logs = torch.log(self.scale/(torch.sqrt(vars)+1e-6))\n",
    "            self.bias.data.copy_(bias.data)\n",
    "            self.logs.data.copy_(logs.data)\n",
    "            self.inited = True\n",
    "\n",
    "    def _center(self, input, reverse=False):\n",
    "        if not reverse:\n",
    "            return input + self.bias\n",
    "        else:\n",
    "            return input - self.bias\n",
    "\n",
    "    def _scale(self, input, logdet=None, reverse=False):\n",
    "        logs = self.logs\n",
    "        if not reverse:\n",
    "            input = input * torch.exp(logs)\n",
    "        else:\n",
    "            input = input * torch.exp(-logs)\n",
    "        if logdet is not None:\n",
    "            \"\"\"\n",
    "            logs is log_std of `mean of channels`\n",
    "            so we need to multiply pixels\n",
    "            \"\"\"\n",
    "            dlogdet = thops_sum(logs) * thops_pixels(input)\n",
    "            if reverse:\n",
    "                dlogdet *= -1\n",
    "            logdet = logdet + dlogdet\n",
    "        return input, logdet\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        if not self.inited:\n",
    "            self.initialize_parameters(input) # use the first input to initialise the params\n",
    "        self._check_input_dim(input)\n",
    "        if not reverse:\n",
    "            # center and scale\n",
    "            input = self._center(input, reverse)\n",
    "            input, logdet = self._scale(input, logdet, reverse)\n",
    "        else:\n",
    "            # scale and center\n",
    "            input, logdet = self._scale(input, logdet, reverse)\n",
    "            input = self._center(input, reverse)\n",
    "        return input, logdet\n",
    "\n",
    "\n",
    "class ActNorm2d(_ActNorm):\n",
    "    def __init__(self, num_features, scale=1.):\n",
    "        super().__init__(num_features, scale) # Send the argument \n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        assert len(input.size()) == 4\n",
    "        assert input.size(1) == self.num_features, (\n",
    "            \"[ActNorm]: input should be in shape as `BCHW`,\"\n",
    "            \" channels should be {} rather than {}\".format(\n",
    "                self.num_features, input.size()))\n",
    "\n",
    "\n",
    "class LinearZeros(nn.Linear):\n",
    "    def __init__(self, in_channels, out_channels, logscale_factor=3):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.logscale_factor = logscale_factor\n",
    "        # set logs parameter\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(out_channels)))\n",
    "        # init\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input) # use the original forward (inherited forward)\n",
    "        return output * torch.exp(self.logs * self.logscale_factor)\n",
    "\n",
    "\n",
    "class Conv2d(nn.Conv2d):\n",
    "    pad_dict = {\n",
    "        \"same\": lambda kernel, stride: [((k - 1) * s + 1) // 2 for k, s in zip(kernel, stride)],\n",
    "        \"valid\": lambda kernel, stride: [0 for _ in kernel]\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_padding(padding, kernel_size, stride):\n",
    "        # make paddding\n",
    "        if isinstance(padding, str):\n",
    "            if isinstance(kernel_size, int):\n",
    "                kernel_size = [kernel_size, kernel_size]\n",
    "            if isinstance(stride, int):\n",
    "                stride = [stride, stride]\n",
    "            padding = padding.lower()\n",
    "            try:\n",
    "                padding = Conv2d.pad_dict[padding](kernel_size, stride)\n",
    "            except KeyError:\n",
    "                raise ValueError(\"{} is not supported\".format(padding))\n",
    "        return padding\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1],\n",
    "                 padding=\"same\", do_actnorm=True, weight_std=0.05):\n",
    "        padding = Conv2d.get_padding(padding, kernel_size, stride)\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                         padding, bias=(not do_actnorm))\n",
    "        # init weight with std\n",
    "        self.weight.data.normal_(mean=0.0, std=weight_std) # it can get the params from the inherited class\n",
    "        if not do_actnorm:\n",
    "            self.bias.data.zero_()\n",
    "        else:\n",
    "            self.actnorm = ActNorm2d(out_channels)\n",
    "        self.do_actnorm = do_actnorm\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = super().forward(input)\n",
    "        if self.do_actnorm:\n",
    "            x, _ = self.actnorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dZeros(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1],\n",
    "                 padding=\"same\", logscale_factor=3):\n",
    "        padding = Conv2d.get_padding(padding, kernel_size, stride)\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # logscale_factor\n",
    "        self.logscale_factor = logscale_factor\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(out_channels, 1, 1))) # learnable\n",
    "        # if the class has the nn.Module base, then it can use self.register_parameter\n",
    "        # init\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        return output * torch.exp(self.logs * self.logscale_factor)\n",
    "\n",
    "\n",
    "class Permute2d(nn.Module):\n",
    "    '''\n",
    "    Will be used when the FlowPermutation is shuffle rather than inconv\n",
    "    Can we just use z = z[:,::-1,:,:]? forward\n",
    "    '''\n",
    "    def __init__(self, num_channels, shuffle):\n",
    "        \n",
    "    \n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.indices = np.arange(self.num_channels - 1, -1, -1).astype(np.long)\n",
    "        self.indices_inverse = np.zeros((self.num_channels), dtype=np.long)\n",
    "        for i in range(self.num_channels):\n",
    "            self.indices_inverse[self.indices[i]] = i\n",
    "        if shuffle:\n",
    "            self.reset_indices()\n",
    "\n",
    "    def reset_indices(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        for i in range(self.num_channels):\n",
    "            self.indices_inverse[self.indices[i]] = i\n",
    "\n",
    "    def forward(self, input, reverse=False):\n",
    "        assert len(input.size()) == 4\n",
    "        if not reverse:\n",
    "            return input[:, self.indices, :, :]\n",
    "        else:\n",
    "            return input[:, self.indices_inverse, :, :]\n",
    "        \n",
    "\n",
    "class InvertibleConv1x1(nn.Module):\n",
    "    def __init__(self, num_channels, LU_decomposed=False):\n",
    "        super().__init__()\n",
    "        if not LU_decomposed:\n",
    "            w_shape = [num_channels, num_channels]\n",
    "            # Sample a random orthogonal matrix:\n",
    "            w_init = np.linalg.qr(\n",
    "                np.random.randn(*w_shape))[0].astype(np.float32)\n",
    "            self.register_parameter(\"weight\",\n",
    "                                    nn.Parameter(torch.Tensor(w_init)))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        \"\"\"\n",
    "        log-det = log|abs(|W|)| * pixels\n",
    "        \"\"\"\n",
    "        w_shape = self.weight.size()\n",
    "        pixels = thops_pixels(input)\n",
    "        dlogdet = torch.log(torch.abs(torch.det(self.weight))) * pixels\n",
    "        if not reverse:\n",
    "            weight = self.weight.view(w_shape[0], w_shape[1], 1, 1)\n",
    "            z = F.conv2d(input, weight)\n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "            return z, logdet\n",
    "        else:\n",
    "            weight = torch.inverse(self.weight).view(w_shape[0], w_shape[1], 1, 1)\n",
    "            z = F.conv2d(input, weight)\n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class GaussianDiag: \n",
    "    Log2PI = float(np.log(2 * np.pi)) # a float, not np.array\n",
    " \n",
    "    @staticmethod\n",
    "    def likelihood(mean, logs, x):\n",
    "        \"\"\"\n",
    "        lnL = -1/2 * { ln|Var| + ((X - Mu)^T)(Var^-1)(X - Mu) + kln(2*PI) }\n",
    "              k = 1 (Independent)\n",
    "              Var = logs ** 2\n",
    "        \"\"\"\n",
    "        part0 = logs * 2.\n",
    "        part1 = ((x - mean) ** 2) / torch.exp(logs * 2.)\n",
    "        part2 = GaussianDiag.Log2PI\n",
    "        return -0.5 * (part0 + part1 + part2)\n",
    "\n",
    "    @staticmethod\n",
    "    def logp(mean, logs, x):\n",
    "        '''\n",
    "        log probability: p(x)\n",
    "        '''\n",
    "        likelihood = GaussianDiag.likelihood(mean, logs, x)\n",
    "        return thops_sum(likelihood, dim=[1, 2, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(mean, logs, eps_std=None):\n",
    "\n",
    "        mean_size = [int(d) for d in mean.size()]\n",
    "        if eps_std is not None:\n",
    "            eps = torch.Tensor(np.random.normal(0, eps_std, mean_size)).to(mean.device)\n",
    "        else:\n",
    "            eps = torch.Tensor(np.random.normal(0, 1, mean_size)).to(mean.device)\n",
    "        return mean + torch.exp(logs) * eps\n",
    "\n",
    "\n",
    "class Split2d(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        '''\n",
    "        C -> C//2\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv = Conv2dZeros(num_channels // 2, num_channels)\n",
    "\n",
    "    def split2d_prior(self, z):\n",
    "        h = self.conv(z)\n",
    "        return thops_split_feature(h, \"cross\")\n",
    "\n",
    "    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n",
    "        if not reverse:\n",
    "            z1, z2 = thops_split_feature(input, \"split\")\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            logdet = GaussianDiag.logp(mean, logs, z2) + logdet\n",
    "            return z1, logdet\n",
    "        else:\n",
    "            z1 = input\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            z2 = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            z = thops_cat_feature(z1, z2)\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "def squeeze2d(input, factor=2):\n",
    "    assert factor >= 1 and isinstance(factor, int)\n",
    "    if factor == 1:\n",
    "        return input\n",
    "    size = input.size()\n",
    "    B = size[0]\n",
    "    C = size[1]\n",
    "    H = size[2]\n",
    "    W = size[3]\n",
    "    assert H % factor == 0 and W % factor == 0, \"{}\".format((H, W))\n",
    "    x = input.view(B, C, H // factor, factor, W // factor, factor)\n",
    "    x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "    x = x.view(B, C * factor * factor, H // factor, W // factor)\n",
    "    return x\n",
    "\n",
    "\n",
    "def unsqueeze2d(input, factor=2):\n",
    "    '''\n",
    "    F.pixelshuffle(input ,2)\n",
    "    '''\n",
    "    assert factor >= 1 and isinstance(factor, int)\n",
    "    factor2 = factor ** 2\n",
    "    if factor == 1:\n",
    "        return input\n",
    "    size = input.size()\n",
    "    B = size[0]\n",
    "    C = size[1]\n",
    "    H = size[2]\n",
    "    W = size[3]\n",
    "    assert C % (factor2) == 0, \"{}\".format(C)\n",
    "    x = input.view(B, C // factor2, factor, factor, H, W)\n",
    "    x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
    "    x = x.view(B, C // (factor2), H * factor, W * factor)\n",
    "    return x\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        # no parameters\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            output = squeeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "        else:\n",
    "            output = unsqueeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "\n",
    "def f(in_channels, out_channels, hidden_channels):\n",
    "    '''\n",
    "    2 conv layers with 1 conv0\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        Conv2d(in_channels, hidden_channels), nn.ReLU(inplace=True), \n",
    "        Conv2d(hidden_channels, hidden_channels, kernel_size=[1, 1]), nn.ReLU(inplace=True),\n",
    "        Conv2dZeros(hidden_channels, out_channels))\n",
    "\n",
    " \n",
    "class FlowStep(nn.Module):\n",
    "    FlowCoupling = [\"additive\", \"affine\"]\n",
    "    FlowPermutation = {\n",
    "        \"reverse\": lambda obj, z, logdet, rev: (obj.reverse(z, rev), logdet),\n",
    "        \"shuffle\": lambda obj, z, logdet, rev: (obj.shuffle(z, rev), logdet),\n",
    "        \"invconv\": lambda obj, z, logdet, rev: obj.invconv(z, logdet, rev)\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels,\n",
    "                 actnorm_scale=1.0,\n",
    "                 flow_permutation=\"invconv\",\n",
    "                 flow_coupling=\"additive\",\n",
    "                 LU_decomposed=False):\n",
    "        \n",
    "        '''\n",
    "        1. actnorm\n",
    "        2. flow permutation (inconv) InvertibleConv1x1\n",
    "        3. split z\n",
    "        5. coupling (addictive) f, z2 += self.f(z1)\n",
    "    \n",
    "        '''\n",
    "        # check configures\n",
    "        assert flow_coupling in FlowStep.FlowCoupling,\\\n",
    "            \"flow_coupling should be in `{}`\".format(FlowStep.FlowCoupling)\n",
    "        assert flow_permutation in FlowStep.FlowPermutation,\\\n",
    "            \"float_permutation should be in `{}`\".format(\n",
    "                FlowStep.FlowPermutation.keys())\n",
    "        super().__init__()\n",
    "        self.flow_permutation = flow_permutation\n",
    "        self.flow_coupling = flow_coupling\n",
    "        # 1. actnorm\n",
    "        self.actnorm = ActNorm2d(in_channels, actnorm_scale)\n",
    "        # 2. permute\n",
    "        if flow_permutation == \"invconv\":\n",
    "            self.invconv = InvertibleConv1x1(\n",
    "                in_channels, LU_decomposed=LU_decomposed)\n",
    "        elif flow_permutation == \"shuffle\":\n",
    "            self.shuffle = Permute2d(in_channels, shuffle=True)\n",
    "        else:\n",
    "            self.reverse = Permute2d(in_channels, shuffle=False)\n",
    "        # 3. coupling\n",
    "        if flow_coupling == \"additive\":\n",
    "            self.f = f(in_channels // 2, in_channels // 2, hidden_channels)\n",
    "        elif flow_coupling == \"affine\":\n",
    "            self.f = f(in_channels // 2, in_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            return self.normal_flow(input, logdet)\n",
    "        else:\n",
    "            return self.reverse_flow(input, logdet)\n",
    "\n",
    "    def normal_flow(self, input, logdet):\n",
    "        assert input.size(1) % 2 == 0\n",
    "        # 1. actnorm\n",
    "        z, logdet = self.actnorm(input, logdet=logdet, reverse=False)\n",
    "        # 2. permute\n",
    "        z, logdet = FlowStep.FlowPermutation[self.flow_permutation](\n",
    "            self, z, logdet, False)\n",
    "        # 3. coupling\n",
    "        z1, z2 = thops_split_feature(z, \"split\")\n",
    "        if self.flow_coupling == \"additive\":\n",
    "            z2 += self.f(z1)\n",
    "        elif self.flow_coupling == \"affine\":\n",
    "            h = self.f(z1)\n",
    "            shift, scale = thops_split_feature(h, \"cross\")\n",
    "            scale = F.sigmoid(scale + 2.)\n",
    "            z2 += shift\n",
    "            z2 *= scale\n",
    "            logdet = thops_sum(torch.log(scale), dim=[1, 2, 3]) + logdet\n",
    "        z = thops_cat_feature(z1, z2)\n",
    "        return z, logdet\n",
    "\n",
    "    def reverse_flow(self, input, logdet):\n",
    "        assert input.size(1) % 2 == 0\n",
    "        # 1.coupling\n",
    "        z1, z2 = thops_split_feature(input, \"split\")\n",
    "        if self.flow_coupling == \"additive\":\n",
    "            z2 -= self.f(z1)\n",
    "        elif self.flow_coupling == \"affine\":\n",
    "            h = self.f(z1)\n",
    "            shift, scale = thops_split_feature(h, \"cross\")\n",
    "            scale = F.sigmoid(scale + 2.)\n",
    "            z2 /= scale\n",
    "            z2 -= shift\n",
    "            logdet = -thops_sum(torch.log(scale), dim=[1, 2, 3]) + logdet\n",
    "        z = thops_cat_feature(z1, z2)\n",
    "        # 2. permute\n",
    "        z, logdet = FlowStep.FlowPermutation[self.flow_permutation](\n",
    "            self, z, logdet, True)\n",
    "        # 3. actnorm\n",
    "        z, logdet = self.actnorm(z, logdet=logdet, reverse=True)\n",
    "        return z, logdet\n",
    "\n",
    "\n",
    "class FlowNet(nn.Module):\n",
    "    def __init__(self, image_shape, hidden_channels, K, L,\n",
    "                 actnorm_scale=1.0,\n",
    "                 flow_permutation=\"invconv\",\n",
    "                 flow_coupling=\"additive\",\n",
    "                 LU_decomposed=False):\n",
    "        \"\"\"\n",
    "                             K                                      K\n",
    "        --> [Squeeze] -> [FlowStep] -> [Split] -> [Squeeze] -> [FlowStep]\n",
    "               ^                           v\n",
    "               |          (L - 1)          |\n",
    "               + --------------------------+\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.output_shapes = []\n",
    "        self.K = K\n",
    "        self.L = L\n",
    "        C, H, W = image_shape # change this\n",
    "        assert C == 1 or C == 3, (\"image_shape should be CHW, like (3, 64, 64)\"\n",
    "                                  \"C == 1 or C == 3\")\n",
    "        for i in range(L):\n",
    "            # 1. Squeeze\n",
    "            C, H, W = C * 4, H // 2, W // 2\n",
    "            self.layers.append(SqueezeLayer(factor=2))\n",
    "            self.output_shapes.append([-1, C, H, W])\n",
    "            # 2. K FlowStep\n",
    "            for _ in range(K):\n",
    "                self.layers.append(\n",
    "                    FlowStep(in_channels=C,\n",
    "                             hidden_channels=hidden_channels,\n",
    "                             actnorm_scale=actnorm_scale,\n",
    "                             flow_permutation=flow_permutation,\n",
    "                             flow_coupling=flow_coupling,\n",
    "                             LU_decomposed=LU_decomposed))\n",
    "                self.output_shapes.append(\n",
    "                    [-1, C, H, W]) # the flowstep won't change the shape\n",
    "            # 3. Split2d\n",
    "            if i < L - 1:\n",
    "                self.layers.append(Split2d(num_channels=C))\n",
    "                self.output_shapes.append([-1, C // 2, H, W])\n",
    "                C = C // 2\n",
    "\n",
    "    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n",
    "        if not reverse:\n",
    "            return self.encode(input, logdet)\n",
    "        else:\n",
    "            return self.decode(input, eps_std)\n",
    "\n",
    "    def encode(self, z, logdet=0.0):\n",
    "        for layer, shape in zip(self.layers, self.output_shapes): # Why do we need to use the output_shape\n",
    "            z, logdet = layer(z, logdet, reverse=False)\n",
    "        return z, logdet\n",
    "\n",
    "    def decode(self, z, eps_std=None):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Split2d):\n",
    "                z, logdet = layer(z, logdet=0, reverse=True, eps_std=eps_std)\n",
    "            else:\n",
    "                z, logdet = layer(z, logdet=0, reverse=True)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Glow(nn.Module):\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.flow = FlowNet(image_shape=hparams.Glow.image_shape,\n",
    "                            hidden_channels=hparams.Glow.hidden_channels,\n",
    "                            K=hparams.Glow.K,\n",
    "                            L=hparams.Glow.L,\n",
    "                            actnorm_scale=hparams.Glow.actnorm_scale,\n",
    "                            flow_permutation=hparams.Glow.flow_permutation,\n",
    "                            flow_coupling=hparams.Glow.flow_coupling,\n",
    "                            LU_decomposed=hparams.Glow.LU_decomposed)\n",
    "        self.hparams = hparams\n",
    "        self.y_classes = hparams.Glow.y_classes\n",
    "        # for prior\n",
    "        if hparams.Glow.learn_top:\n",
    "            C = self.flow.output_shapes[-1][1]\n",
    "            self.learn_top = Conv2dZeros(C * 2, C * 2) \n",
    "        if hparams.Glow.y_condition:\n",
    "            C = self.flow.output_shapes[-1][1]\n",
    "            self.project_ycond = LinearZeros(\n",
    "                hparams.Glow.y_classes, 2 * C)\n",
    "            self.project_class = LinearZeros(\n",
    "                C, hparams.Glow.y_classes)\n",
    "        # register prior hidden\n",
    "\n",
    "        self.register_parameter(\n",
    "            \"prior_h\",\n",
    "            nn.Parameter(torch.zeros([hparams.Train.batch_size,\n",
    "                                      self.flow.output_shapes[-1][1] * 2,  # the last output shape\n",
    "                                      self.flow.output_shapes[-1][2],\n",
    "                                      self.flow.output_shapes[-1][3]])))\n",
    "        \n",
    "        # Q: The shape of prior_h is the same as the last output? self.flow.output_shapes[-1]\n",
    "\n",
    "    def prior(self, y_onehot=None):\n",
    "        # prior distribution\n",
    "        B, C = self.prior_h.size(0), self.prior_h.size(1)\n",
    "        h = self.prior_h.detach().clone() # prior_h won't be changed\n",
    "        assert torch.sum(h) == 0.0 # the h will\n",
    "        if self.hparams.Glow.learn_top:\n",
    "            h = self.learn_top(h) # Conv2dZeros(C * 2, C * 2) add one more layer, learn the presentation of h\n",
    "        if self.hparams.Glow.y_condition:\n",
    "            assert y_onehot is not None\n",
    "            yp = self.project_ycond(y_onehot).view(B, C, 1, 1) # LinearZeros(hparams.Glow.y_classes, 2 * C)\n",
    "            h += yp\n",
    "        return thops_split_feature(h, \"split\")\n",
    "\n",
    "    def forward(self, x=None, y_onehot=None, z=None,\n",
    "                eps_std=None, reverse=False):\n",
    "        if not reverse:\n",
    "            return self.normal_flow(x, y_onehot)\n",
    "        else:\n",
    "            return self.reverse_flow(z, y_onehot, eps_std)\n",
    "\n",
    "    def normal_flow(self, x, y_onehot):\n",
    "        '''\n",
    "        Forward:\n",
    "        \n",
    "        1. add noise\n",
    "        2. initialise the logdet\n",
    "        3. forward flow\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        pixels = thops_pixels(x)\n",
    "        z = x + torch.normal(mean=torch.zeros_like(x),  # add noise to the input.\n",
    "                             std=torch.ones_like(x) * (1. / 256.))  \n",
    "        logdet = torch.zeros_like(x[:, 0, 0, 0]) # Create an initialised logdet\n",
    "        logdet += float(-np.log(256.) * pixels)\n",
    "        # encode\n",
    "        z, objective = self.flow(z, logdet=logdet, reverse=False)\n",
    "        # prior <The meaning of prior distribution>, A: create mean and std\n",
    "        mean, logs = self.prior(y_onehot) # do prior after self.flow \n",
    "        objective += GaussianDiag.logp(mean, logs, z) # log det + log p(x)\n",
    "\n",
    "\n",
    "        if self.hparams.Glow.y_condition:\n",
    "            y_logits = self.project_class(z.mean(2).mean(2))\n",
    "        else:\n",
    "            y_logits = None\n",
    "\n",
    "        # return\n",
    "        nll = (-objective) / float(np.log(2.) * pixels) # Q: nll is -logdet?\n",
    "        return z, nll, y_logits\n",
    "\n",
    "    def reverse_flow(self, z, y_onehot, eps_std):\n",
    "        with torch.no_grad():\n",
    "            mean, logs = self.prior(y_onehot)\n",
    "            if z is None:\n",
    "                z = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            x = self.flow(z, eps_std=eps_std, reverse=True)\n",
    "        return x\n",
    "\n",
    "    def set_actnorm_init(self, inited=True):\n",
    "        for name, m in self.named_modules():\n",
    "            if (m.__class__.__name__.find(\"ActNorm\") >= 0):\n",
    "                m.inited = inited\n",
    "                \n",
    "    def sample(self, temperatures = [0.25]):\n",
    "        #[0., .25, .5, .6, .7, .8, .9, 1.]\n",
    "        x_sample = []\n",
    "        y = ((list(np.arange(0,hparams.Glow.y_classes))*\\\n",
    "              ((hparams.Train.batch_size)//hparams.Glow.y_classes + 1)))[:hparams.Train.batch_size]\n",
    "        y = torch.LongTensor(y).to(device)\n",
    "        for i in temperatures:\n",
    "            x_sample.append(self.reverse_flow(None,\n",
    "                                              thops_onehot(y, hparams.Glow.y_classes)[:64].to(device),\n",
    "                                              i))\n",
    "        return torch.stack(x_sample)                \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def loss_generative(nll):\n",
    "        # Generative loss\n",
    "        return torch.mean(nll)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_multi_classes(y_logits, y_onehot):\n",
    "        if y_logits is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return Glow.BCE(y_logits, y_onehot.float())\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_class(y_logits, y):\n",
    "        if y_logits is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return Glow.CE(y_logits, y.long())\n",
    "  \n",
    "    def image_save(self, img, step):\n",
    "        \n",
    "        path = './' + hparams.Glow.model_name+ '/Img/' + hparams.Glow.model_name +'_Step_' + str(step) + '.png'\n",
    "        save_image( img, path , nrow=8, normalize=True, range=(-1,1))\n",
    "        print('Image saved')  \n",
    " \n",
    "    def model_save(self, step):\n",
    "        path = './' + hparams.Glow.model_name+ '/Model/' + hparams.Glow.model_name +'_Step_' + str(step) + '.pth'\n",
    "        torch.save({hparams.Glow.model_name :self.state_dict()}, path)\n",
    "        print('Model saved')\n",
    "   \n",
    "    def load_step_dict(self,step):\n",
    "        \n",
    "        path = './' + hparams.Glow.model_name+ '/Model/' + hparams.Glow.model_name +'_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage)[hparams.Glow.model_name])\n",
    "        \n",
    "    def num_all_params(self,):\n",
    "        return sum([param.nelement() for param in self.parameters()])\n",
    "    \n",
    "\n",
    "    def plot_all_loss(self, train_hist, step):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (20,8))\n",
    "        for k in train_hist.keys():\n",
    "            plt.plot(train_hist[k], label= k)\n",
    "        plt.ylabel('Loss',fontsize=15)\n",
    "        plt.xlabel('Number of Steps',fontsize=15)\n",
    "        plt.title('Loss',fontsize=30,fontweight =\"bold\")\n",
    "        plt.legend(loc = 'upper left')\n",
    "        fig.savefig( hparams.Glow.model_name +\"_Loss_\"+str(step)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "to_img= T.Compose([T.ToPILImage()])\n",
    "to_tensor = T.Compose([T.ToTensor()])\n",
    "load_norm = T.Compose([#T.CenterCrop(hparams.Data.img_size,hparams.Data.img_size),\n",
    "                       T.Resize((hparams.Data.img_size,hparams.Data.img_size)),\n",
    "                       T.ToTensor(),\n",
    "                       T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ])\n",
    "\n",
    "if hparams.Data.name == 'celeba':\n",
    "    dataset = CelebADataset(mode='train')\n",
    "    training_loader = DataLoader(dataset,batch_size=hparams.Train.batch_size,shuffle=True,drop_last=True,pin_memory=True)\n",
    "elif hparams.Data.name == 'mnist':\n",
    "    training_loader = DataLoader(datasets.MNIST('./data/mnist',train=True,\n",
    "                    download=True,transform=load_norm),\n",
    "                    batch_size=hparams.Train.batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "all_steps = 1\n",
    "train_hist = {}\n",
    "train_hist['G_Loss'] = []\n",
    "train_hist['C_Loss'] = []\n",
    "glow = Glow(hparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(glow.parameters(), lr = hparams.Optim.lr, betas=(hparams.Optim.beta1, hparams.Optim.beta2))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,10000,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step [1] | lr [0.0010] | G_Loss: [3.393] | C_Loss: [0.693] | Time: 102.6s\n",
      "| Step [2] | lr [0.0010] | G_Loss: [33.711] | C_Loss: [0.686] | Time: 93.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-167210859356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_generative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_classes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mend_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < hparams.Train.n_epoch:\n",
    "    for i,(img, y) in enumerate(training_loader):\n",
    "        start_t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        y = y.to(device)\n",
    "        #scheduler.step()\n",
    "        z, nll, y_logits = glow(img.to(device), thops_onehot(y, hparams.Glow.y_classes).to(device))\n",
    "        loss_generative = glow.loss_generative(nll)\n",
    "        loss_classes = glow.loss_class(y_logits, y)\n",
    "        loss = loss_generative + loss_classes * hparams.Train.weight_y\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_t = time.time()\n",
    "        train_hist['G_Loss'].append(loss_generative.item())\n",
    "        train_hist['C_Loss'].append(loss_classes.item())\n",
    "        \n",
    "        print('| Step [%d] | lr [%.4f] | G_Loss: [%.3f] | C_Loss: [%.3f] | Time: %.1fs' %\\\n",
    "              ( all_steps, optimizer.param_groups[0]['lr'], loss_generative.item(), loss_classes.item(),\n",
    "               end_t - start_t))\n",
    "        \n",
    "        if all_steps % hparams.Train.img_show_freq == 0: #hparams.Train.img_show_freq\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            fig.add_subplot(1,3,1)\n",
    "            sample_img = glow.sample().squeeze(0)\n",
    "            plt.imshow(to_img(sample_img[0].cpu()*0.5+0.5))\n",
    "            plt.show()\n",
    "            if all_steps % hparams.Train.img_save_freq == 0: # hparams.Train.img_save_freq\n",
    "                glow.plot_all_loss(train_hist, 'Training')\n",
    "                glow.image_save(sample_img,all_steps)\n",
    "                if all_steps % hparams.Train.model_save_freq == 0: # hparams.Train.model_save_freq\n",
    "                    glow.model_save(all_steps)\n",
    "                    \n",
    "        all_steps += 1\n",
    "        if all_steps > 1000:\n",
    "            raise StopIteration\n",
    "    epoch +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
